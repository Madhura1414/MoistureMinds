{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom datetime import datetime\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom sklearn.preprocessing import MinMaxScaler\nimport xgboost as xg","metadata":{"execution":{"iopub.status.busy":"2023-03-26T06:26:22.367747Z","iopub.execute_input":"2023-03-26T06:26:22.368176Z","iopub.status.idle":"2023-03-26T06:26:22.472627Z","shell.execute_reply.started":"2023-03-26T06:26:22.368140Z","shell.execute_reply":"2023-03-26T06:26:22.471122Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Load data\nurl1 = \"/kaggle/input/moistureminds22/user1_data.csv\"\nurl2 = \"/kaggle/input/moistureminds22/user2_data.csv\"\n\ndf1 = pd.read_csv(url1)\ndf2 = pd.read_csv(url2)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T05:32:21.681991Z","iopub.execute_input":"2023-03-26T05:32:21.682781Z","iopub.status.idle":"2023-03-26T05:32:21.740865Z","shell.execute_reply.started":"2023-03-26T05:32:21.682737Z","shell.execute_reply":"2023-03-26T05:32:21.739257Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\n# Handle missing values\ndf1 = df1.interpolate(method='linear', limit_direction='both')\ndf2 = df2.interpolate(method='linear', limit_direction='both')\ndf1.fillna(method='bfill', inplace=True)\ndf2.fillna(method='bfill', inplace=True)\n\ndf1.info()\ndf2.info()","metadata":{"execution":{"iopub.status.busy":"2023-03-26T05:32:27.100353Z","iopub.execute_input":"2023-03-26T05:32:27.100803Z","iopub.status.idle":"2023-03-26T05:32:27.133066Z","shell.execute_reply.started":"2023-03-26T05:32:27.100764Z","shell.execute_reply":"2023-03-26T05:32:27.131598Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 19341 entries, 0 to 19340\nData columns (total 8 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   ttime   19341 non-null  object \n 1   pm1     19341 non-null  float64\n 2   pm2     19341 non-null  float64\n 3   pm3     19341 non-null  float64\n 4   am      19341 non-null  float64\n 5   sm      19341 non-null  float64\n 6   st      19341 non-null  float64\n 7   lum     19341 non-null  float64\ndtypes: float64(7), object(1)\nmemory usage: 1.2+ MB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 20166 entries, 0 to 20165\nData columns (total 10 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   ttime   20166 non-null  object \n 1   pm1     20166 non-null  float64\n 2   pm2     20166 non-null  float64\n 3   pm3     20166 non-null  float64\n 4   am      20166 non-null  float64\n 5   sm      20166 non-null  float64\n 6   lum     20166 non-null  float64\n 7   temp    20166 non-null  float64\n 8   humd    20166 non-null  float64\n 9   pres    20166 non-null  float64\ndtypes: float64(9), object(1)\nmemory usage: 1.5+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"# Convert the \"ttime\" column to datetime\ndf1[\"ttime\"] = pd.to_datetime(df1[\"ttime\"], errors='coerce')\n\n# Convert the datetime values to Unix timestamp (number of seconds since January 1, 1970)\ndf1[\"ttime\"] = df1[\"ttime\"].apply(lambda x: x.timestamp())\n\n# Convert the \"ttime\" column to float\ndf1[\"ttime\"] = pd.to_numeric(df1[\"ttime\"], errors='coerce').astype(float)\n\n# Convert the \"ttime\" column to datetime\ndf2[\"ttime\"] = pd.to_datetime(df2[\"ttime\"], errors='coerce')\n\n# Convert the datetime values to Unix timestamp (number of seconds since January 1, 1970)\ndf2[\"ttime\"] = df2[\"ttime\"].apply(lambda x: x.timestamp())\n\n# Convert the \"ttime\" column to float\ndf2[\"ttime\"] = pd.to_numeric(df2[\"ttime\"], errors='coerce').astype(float)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T05:35:03.442842Z","iopub.execute_input":"2023-03-26T05:35:03.443307Z","iopub.status.idle":"2023-03-26T05:35:03.762332Z","shell.execute_reply.started":"2023-03-26T05:35:03.443267Z","shell.execute_reply":"2023-03-26T05:35:03.760999Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df2['ttime']","metadata":{"execution":{"iopub.status.busy":"2023-03-26T05:41:16.358029Z","iopub.execute_input":"2023-03-26T05:41:16.359216Z","iopub.status.idle":"2023-03-26T05:41:16.367110Z","shell.execute_reply.started":"2023-03-26T05:41:16.359169Z","shell.execute_reply":"2023-03-26T05:41:16.365829Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"0        1.658159e+09\n1        1.658159e+09\n2        1.658159e+09\n3        1.658160e+09\n4        1.658160e+09\n             ...     \n20161    1.678439e+09\n20162    1.678440e+09\n20163    1.678442e+09\n20164    1.678444e+09\n20165    1.678445e+09\nName: ttime, Length: 20166, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"df1['ttime']","metadata":{"execution":{"iopub.status.busy":"2023-03-26T05:41:57.189414Z","iopub.execute_input":"2023-03-26T05:41:57.189842Z","iopub.status.idle":"2023-03-26T05:41:57.200839Z","shell.execute_reply.started":"2023-03-26T05:41:57.189790Z","shell.execute_reply":"2023-03-26T05:41:57.199326Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"0        1.658159e+09\n1        1.658159e+09\n2        1.658160e+09\n3        1.658160e+09\n4        1.658160e+09\n             ...     \n19336    1.678438e+09\n19337    1.678440e+09\n19338    1.678441e+09\n19339    1.678443e+09\n19340    1.678446e+09\nName: ttime, Length: 19341, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"# Normalize the data\nscaler1 = StandardScaler()\nscaler2 = StandardScaler()\n\n#df1[df1.columns[1:]] = scaler1.fit_transform(df1[df1.columns[1:]])\n#df2[df2.columns[1:]] = scaler2.fit_transform(df2[df2.columns[1:]])\n\nscaler = StandardScaler(with_mean=True, with_std=True)\nscaler.n_features_in_ = 9","metadata":{"execution":{"iopub.status.busy":"2023-03-26T05:35:10.514854Z","iopub.execute_input":"2023-03-26T05:35:10.515282Z","iopub.status.idle":"2023-03-26T05:35:10.521489Z","shell.execute_reply.started":"2023-03-26T05:35:10.515245Z","shell.execute_reply":"2023-03-26T05:35:10.520106Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Merge the datasets\n#df = pd.merge(df1, df2, on=['latitude', 'longitude', 'date'], how='inner')\ndf = pd.concat([df1, df2], axis=0)\ndf[df.columns[1:]] = scaler1.fit_transform(df[df.columns[1:]])\n# scaler1.fit(X_train)\n# X.shape[1]\n# print(\"ffddggs\")\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T05:42:39.433912Z","iopub.execute_input":"2023-03-26T05:42:39.434785Z","iopub.status.idle":"2023-03-26T05:42:39.480419Z","shell.execute_reply.started":"2023-03-26T05:42:39.434748Z","shell.execute_reply":"2023-03-26T05:42:39.479248Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"          ttime       pm1       pm2       pm3        am        sm        st  \\\n0  1.658159e+09  1.931896 -2.019029 -2.115044  0.857986  1.168879 -1.033500   \n1  1.658159e+09  2.464284 -2.019029 -2.115044  1.603569  1.128451 -1.024907   \n2  1.658160e+09  2.987165 -2.019029 -2.115044  1.230777  1.168879 -1.016314   \n3  1.658160e+09  3.519553 -2.019029 -2.115044  1.603569  1.168879 -1.007721   \n4  1.658160e+09 -0.996237  2.085582 -1.927497  0.857986  1.168879 -0.999128   \n5  1.658160e+09 -0.463849  2.085582 -1.927497  1.230777  1.168879 -0.981942   \n6  1.658161e+09  0.068539  2.085582 -1.927497  0.112403  1.168879 -0.973349   \n7  1.658161e+09  0.600927  2.085582 -1.927497  2.349152  1.168879 -0.964755   \n8  1.658161e+09  1.133314  2.085582 -1.927497  0.112403  1.168879 -0.956162   \n9  1.658162e+09  1.665702  2.085582 -1.927497  0.857986  1.128451 -0.947569   \n\n        lum  temp  humd  pres  \n0  2.865840   NaN   NaN   NaN  \n1  2.598431   NaN   NaN   NaN  \n2  2.580808   NaN   NaN   NaN  \n3  2.720621   NaN   NaN   NaN  \n4  2.776547   NaN   NaN   NaN  \n5  2.579398   NaN   NaN   NaN  \n6  2.320684   NaN   NaN   NaN  \n7  2.213768   NaN   NaN   NaN  \n8  1.810305   NaN   NaN   NaN  \n9  1.700334   NaN   NaN   NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ttime</th>\n      <th>pm1</th>\n      <th>pm2</th>\n      <th>pm3</th>\n      <th>am</th>\n      <th>sm</th>\n      <th>st</th>\n      <th>lum</th>\n      <th>temp</th>\n      <th>humd</th>\n      <th>pres</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.658159e+09</td>\n      <td>1.931896</td>\n      <td>-2.019029</td>\n      <td>-2.115044</td>\n      <td>0.857986</td>\n      <td>1.168879</td>\n      <td>-1.033500</td>\n      <td>2.865840</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.658159e+09</td>\n      <td>2.464284</td>\n      <td>-2.019029</td>\n      <td>-2.115044</td>\n      <td>1.603569</td>\n      <td>1.128451</td>\n      <td>-1.024907</td>\n      <td>2.598431</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.658160e+09</td>\n      <td>2.987165</td>\n      <td>-2.019029</td>\n      <td>-2.115044</td>\n      <td>1.230777</td>\n      <td>1.168879</td>\n      <td>-1.016314</td>\n      <td>2.580808</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.658160e+09</td>\n      <td>3.519553</td>\n      <td>-2.019029</td>\n      <td>-2.115044</td>\n      <td>1.603569</td>\n      <td>1.168879</td>\n      <td>-1.007721</td>\n      <td>2.720621</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.658160e+09</td>\n      <td>-0.996237</td>\n      <td>2.085582</td>\n      <td>-1.927497</td>\n      <td>0.857986</td>\n      <td>1.168879</td>\n      <td>-0.999128</td>\n      <td>2.776547</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1.658160e+09</td>\n      <td>-0.463849</td>\n      <td>2.085582</td>\n      <td>-1.927497</td>\n      <td>1.230777</td>\n      <td>1.168879</td>\n      <td>-0.981942</td>\n      <td>2.579398</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1.658161e+09</td>\n      <td>0.068539</td>\n      <td>2.085582</td>\n      <td>-1.927497</td>\n      <td>0.112403</td>\n      <td>1.168879</td>\n      <td>-0.973349</td>\n      <td>2.320684</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1.658161e+09</td>\n      <td>0.600927</td>\n      <td>2.085582</td>\n      <td>-1.927497</td>\n      <td>2.349152</td>\n      <td>1.168879</td>\n      <td>-0.964755</td>\n      <td>2.213768</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1.658161e+09</td>\n      <td>1.133314</td>\n      <td>2.085582</td>\n      <td>-1.927497</td>\n      <td>0.112403</td>\n      <td>1.168879</td>\n      <td>-0.956162</td>\n      <td>1.810305</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1.658162e+09</td>\n      <td>1.665702</td>\n      <td>2.085582</td>\n      <td>-1.927497</td>\n      <td>0.857986</td>\n      <td>1.128451</td>\n      <td>-0.947569</td>\n      <td>1.700334</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Fill NaN values in the 'ttime' column with an empty string\ndf1['ttime'] = df1['ttime'].fillna('')\n\n# Convert the 'ttime' column to string type\ndf1['ttime'] = df1['ttime'].astype(str)\n\n# Loop over each value in the 'ttime' column\nfor value in df1['ttime']:\n    # Parse the string into a datetime object\n    dt_obj = datetime.fromtimestamp(float(value))\n    \n    # Extract the Unix timestamp\n    ttimep = dt_obj.timestamp()\n    \n    # Print the timestamp\n    #print(ttimep)\n    \n# Fill NaN values in the 'ttime' column with an empty string\ndf2['ttime'] = df2['ttime'].fillna('')\n\n# Convert the 'ttime' column to string type\ndf2['ttime'] = df2['ttime'].astype(str)\n\n# Loop over each value in the 'ttime' column\nfor value in df2['ttime']:\n    # Parse the string into a datetime object\n    dt_obj = datetime.fromtimestamp(float(value))\n\n    # Extract the Unix timestamp\n    ttimep = dt_obj.timestamp()\n    #print(ttimep)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T06:13:42.269081Z","iopub.execute_input":"2023-03-26T06:13:42.269482Z","iopub.status.idle":"2023-03-26T06:13:42.329888Z","shell.execute_reply.started":"2023-03-26T06:13:42.269447Z","shell.execute_reply":"2023-03-26T06:13:42.328868Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"df['ttime'] = df['ttime'].apply(lambda x: pd.Timestamp(x).timestamp())\n    \n# Split the data into training and testing sets\n#X = df.drop(['sm'], axis=1)\nX = df.drop(['temp','pres'], axis=1)\nX = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\ny = df['sm']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\nprint(\"Before X creation:\", df.shape)\nX = pd.DataFrame(X, columns=df.columns[:-1])\nprint(\"After X creation:\", X.shape)\n\nprint(\"Shape of X_train:\", X_train.shape)\nprint(\"Shape of X_test:\", X_test.shape)\n\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\n\nX_test = pd.DataFrame(X_test, columns=df.columns[:-1])\nX_train = pd.DataFrame(X_train, columns=df.columns[:-1])\nprint(\"shitt\")\nprint(X_test.columns)\nprint(X_train.columns)\n\nprint(\"Number of columns in X_train_scaled_df:\", X_train_scaled.shape[1])\nprint(\"Number of columns in X_test_scaled_df:\", X_test_scaled.shape[1])\n\n# convert X_test_scaled into a dataframe\nX_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n\n\n\n\n# scale X_test_scaled_df using the same scaler used for X_train_scaled\nX_test_scaled_df = scaler.transform(X_test_scaled_df)\n\n# convert X_test_scaled_df back to a numpy array\nX_test_scaled = X_test_scaled_df\n\n#X_test[X_test.columns[1:]] = scaler2.transform(X_test[X_test.columns[1:]])\n","metadata":{"execution":{"iopub.status.busy":"2023-03-26T06:15:30.302631Z","iopub.execute_input":"2023-03-26T06:15:30.303043Z","iopub.status.idle":"2023-03-26T06:15:30.492016Z","shell.execute_reply.started":"2023-03-26T06:15:30.303006Z","shell.execute_reply":"2023-03-26T06:15:30.490706Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Before X creation: (39507, 11)\nAfter X creation: (39507, 10)\nShape of X_train: (27654, 10)\nShape of X_test: (11853, 10)\nshitt\nIndex(['ttime', 'pm1', 'pm2', 'pm3', 'am', 'sm', 'st', 'lum', 'temp', 'humd'], dtype='object')\nIndex(['ttime', 'pm1', 'pm2', 'pm3', 'am', 'sm', 'st', 'lum', 'temp', 'humd'], dtype='object')\nNumber of columns in X_train_scaled_df: 10\nNumber of columns in X_test_scaled_df: 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/base.py:444: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n","output_type":"stream"}]},{"cell_type":"code","source":"\nX_train = X_train.astype('float64')\nX_test = X_test.astype('float64')\ny_train = y_train.astype('float64')\ny_test = y_test.astype('float64')\n\n\n#dfghjklktrdxc cfgb \n# Check for missing values\nX_train.isna().sum()\n\n# Replace missing values with mean of column\nX_train.fillna(X_train.mean(), inplace=True)\n\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Fill NaN values with mean of column\ndf1 = df1.fillna(df1.mean())\n# Fill NaN values with mean of column\ndf2 = df2.fillna(df2.mean())\n\ndf1 = df1.astype('float64')\ndf2 = df2.astype('float64')\n\nscaler = StandardScaler()\nX_test_scaled = scaler.fit_transform(X_test.iloc[:, 1:])\n\nX_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns[1:])\n\nX_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns[1:])\ndf.dropna(inplace=True)\nprint(X_test_scaled_df.isna().sum())\nX_test_scaled_df = X_test_scaled_df.fillna(X_test_scaled_df.mean())\n\nprint(X_test_scaled_df.isna().sum())\n\nprint(X_train.shape)\nprint(X_test.shape)\n\nscaler_train = StandardScaler()\nX_train_scaled = scaler_train.fit_transform(X_train.iloc[:, 1:])\n\nX_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns[1:])\n\nX_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns[1:])","metadata":{"execution":{"iopub.status.busy":"2023-03-26T06:15:36.397033Z","iopub.execute_input":"2023-03-26T06:15:36.397895Z","iopub.status.idle":"2023-03-26T06:15:36.660961Z","shell.execute_reply.started":"2023-03-26T06:15:36.397857Z","shell.execute_reply":"2023-03-26T06:15:36.659543Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"pm1        0\npm2        0\npm3        0\nam         0\nsm         0\nst         0\nlum     6030\ntemp       0\nhumd    5823\ndtype: int64\npm1     0\npm2     0\npm3     0\nam      0\nsm      0\nst      0\nlum     0\ntemp    0\nhumd    0\ndtype: int64\n(27654, 10)\n(11853, 10)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:21: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n","output_type":"stream"}]},{"cell_type":"code","source":"import lightgbm as lb\nfrom lightgbm import LGBMRegressor","metadata":{"execution":{"iopub.status.busy":"2023-03-26T06:31:58.872932Z","iopub.execute_input":"2023-03-26T06:31:58.873762Z","iopub.status.idle":"2023-03-26T06:31:58.879078Z","shell.execute_reply.started":"2023-03-26T06:31:58.873724Z","shell.execute_reply":"2023-03-26T06:31:58.877827Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"\n\n# Linear regression model\nlr = LinearRegression()\nlr_model=lr.fit(X_train_scaled_df, y_train)\nlr_pred = lr.predict(X_test_scaled_df)\n\n# Decision tree regressor model\ndt = DecisionTreeRegressor(random_state=0)\ndt_model=dt.fit(X_train_scaled_df, y_train)\ndt_pred = dt.predict(X_test_scaled_df)\n\n# Random forest regressor model\nrf = RandomForestRegressor(n_estimators=100, random_state=0)\nrf_model=rf.fit(X_train_scaled_df, y_train)\nrf_pred = rf.predict(X_test_scaled_df)\n\nxgb_r = xg.XGBRegressor(objective ='reg:linear',\n                  n_estimators = 100, seed = 123)\nxgb_r.fit(X_train_scaled_df, y_train)\nx_pred = xgb_r.predict(X_test_scaled_df)\n\nlbgm = LGBMRegressor(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=7)\nlbgm.fit(X_train_scaled_df, y_train)\nlb_pred = lbgm.predict(X_test_scaled_df)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T06:33:14.020272Z","iopub.execute_input":"2023-03-26T06:33:14.020671Z","iopub.status.idle":"2023-03-26T06:33:23.807598Z","shell.execute_reply.started":"2023-03-26T06:33:14.020632Z","shell.execute_reply":"2023-03-26T06:33:23.806506Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"[06:33:21] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Ensemble model\nensemble_pred = (lr_pred + dt_pred + rf_pred +x_pred + lb_pred) / 5\n\n# Calculate and print the performance metrics\nensemble_rmse = np.sqrt(mean_squared_error(y_test, ensemble_pred))\nensemble_r2 = r2_score(y_test, ensemble_pred)\n\nprint(\"Ensemble Model Performance Metrics:\")\nprint(\"RMSE:\", ensemble_rmse)\nprint(\"R-squared:\", ensemble_r2)\n\n# ,7813.0,,15001.0\n# 'sm':[2.65]\nlr_pred = lr_model.predict(X_test_scaled_df)\ndt_pred = dt_model.predict(X_test_scaled_df)\nrf_pred = rf_model.predict(X_test_scaled_df)\ndf = pd.DataFrame({'ttime': [1658193598.0], 'pm1': [3.63], 'pm2': [0.0], 'pm3':[0.0], 'am':[7463.0], 'temp':[23.4], 'lum':[2.11],'humd':[90.57], 'pres':[92849.25]})\npredictions = (lr_model.predict(df)+dt_model.predict(df)+rf_model.predict(df) + xgb_r.predict(df)+lbgm.predict(df))/5\nprint(\"Predicted soil moisture:\", predictions)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T06:34:35.377389Z","iopub.execute_input":"2023-03-26T06:34:35.377856Z","iopub.status.idle":"2023-03-26T06:34:35.598932Z","shell.execute_reply.started":"2023-03-26T06:34:35.377805Z","shell.execute_reply":"2023-03-26T06:34:35.597924Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Ensemble Model Performance Metrics:\nRMSE: 0.007414493688587446\nR-squared: 0.9999450891840868\nPredicted soil moisture: [5.64629967]\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\nFeature names unseen at fit time:\n- pres\n- ttime\nFeature names seen at fit time, yet now missing:\n- sm\n- st\n\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.7/site-packages/sklearn/base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\nFeature names unseen at fit time:\n- pres\n- ttime\nFeature names seen at fit time, yet now missing:\n- sm\n- st\n\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.7/site-packages/sklearn/base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\nFeature names unseen at fit time:\n- pres\n- ttime\nFeature names seen at fit time, yet now missing:\n- sm\n- st\n\n  warnings.warn(message, FutureWarning)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}